@article{MARIOTTI2022102873,
title = {The BondMachine, a moldable computer architecture},
journal = {Parallel Computing},
volume = {109},
pages = {102873},
year = {2022},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102873},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121001150},
author = {Mirko Mariotti and Daniel Magalotti and Daniele Spiga and Loriano Storchi},
keywords = {Reconfigurable computing, BondMachine, Hardware/software co-design, Domain specific architectures},
abstract = {Future systems will be characterized by the presence of many computing core in a single device, on large scale data centers or even at the level of IoT devices. The ability to fully exploit computational architectures’ heterogeneity and concurrency will be a key point. In this manuscript we present the BondMachine (BM), an innovative prototype software ecosystem aimed at creating facilities where hardware and software are co-designed, guaranteeing a full exploitation of fabric capabilities (both in terms of concurrency and heterogeneity) with several hardware optimization possibilities. The fundamental innovation of the BM is to provide a new kind of computer architecture, where the hardware dynamically adapts to the specific computational problem rather than being static and generic, as in standard CPUs synthesized in silicon. Hardware can be designed to fit precisely any computational task needs, implementing only the processing units needed and discarding generic solutions. By using BMs within FPGA technologies end-to-end solutions could be realized, in which the creation of domain-specific hardware is part of the development process as much as the software stack. FPGA technology allows to create independent processing units on a single low-power board, and to design their interconnections “in silicon” to maximally fit the design needs. The processors of the BMs are suitable for computational structures like neural networks and tensor processing models. Machine Learning (ML) and Deep Learning (DL) popularity keeps increasing in scientific and industrial areas.}
}
@article{CAPILLA2021106439,
title = {Software engineering and advanced applications conference 2019 – selected papers},
journal = {Information and Software Technology},
volume = {130},
pages = {106439},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106439},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301920},
author = {Rafael Capilla and Miroslaw Staron},
abstract = {Software Engineering and Advanced Applications (SEAA) is a long-standing international forum for researchers, practitioners, and students to present and discuss the latest innovations, trends, experiences, and concerns in the field of Software Engineering and Advanced Applications in information technology for software-intensive systems. In this special issue, we present a selection of papers which show the current trends in software engineering – improved systematic reviews, deep learning and cloud computing.}
}
@article{CADAVID2020106202,
title = {Architecting systems of systems: A tertiary study},
journal = {Information and Software Technology},
volume = {118},
pages = {106202},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106202},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302083},
author = {Héctor Cadavid and Vasilios Andrikopoulos and Paris Avgeriou},
keywords = {Systems of Systems, SoS Architecting, Tertiary study, Systematic literature review},
abstract = {Context: The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent constituent systems that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the Internet of Things.Objective: This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and software quality attributes by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. Method: We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. Results: The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and quality attributes. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. Conclusions: Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.}
}
@article{HERTWIG20191383,
title = {Certification of Openness – Corner Stone of an Agile PLM Strategy},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1383-1391},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.318},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920303851},
author = {Michael Hertwig and Dietmar Trippner and Joachim Lentes},
keywords = {Openness, Certification, agile PLM strategy},
abstract = {The change in products is taking place rapidly, from primarily electromechanical components to cyber physical systems. The associated digitalization in development, production and use is primarily based on data. In order to increase the potentials, a high quality, timeliness and availability of the data is necessary. Due to the increasing number of IT-based systems, interoperability between them is becoming more important. Without the appropriate interfaces and the openness of the systems, additional effort is required. If an appropriate PLM strategy is in current discussion, the principles of "agile strategy development" show high potential. With the further development of the Code of PLM Openness into a certification approach, it is possible to generate a base for an agile product life cycle management. In order to ensure the openness of systems, the developing company must also be in focus.}
}
@article{HOLTMANN2024111943,
title = {Processes, methods, and tools in model-based engineering—A qualitative multiple-case study},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111943},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111943},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003382},
author = {Jörg Holtmann and Grischa Liebel and Jan-Philipp Steghöfer},
keywords = {Model-based engineering, Development processes, Modeling methods},
abstract = {Research on model-based engineering (MBE) has occasionally touched upon the relationship between development processes and concrete MBE practices. However, the alignment of these elements has rarely been the central focus of these studies. As a result, important questions regarding the alignment of MBE and development processes, as well as the impact of development processes on the utilization and success of MBE, have remained unanswered. To address this research gap, we conducted a multiple-case study involving 14 individuals from nine different companies, conducting a total of 12 interviews. Building upon seven propositions derived from existing literature, our investigation sought to understand how MBE is aligned with the development process and explore the application of MBE in this context. Additionally, we identified challenges and needs in this area. Our findings challenge some previously reported results, such as the perceived conflicts between agile development processes and MBE. Furthermore, we unearthed previously unreported issues, like the importance of considering the perspectives of tool vendors in MBE discussions. Overall, this paper makes a significant contribution by providing a comprehensive and up-to-date perspective on how MBE is integrated into development processes, along with an examination of the social and organizational aspects inherent to these processes. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{LIAO2024,
title = {Digital Transformation and Innovation and Business Ecosystems: A Bibliometric Analysis for Conceptual Insights and Collaborative Practices for Ecosystem Innovation},
journal = {International Journal of Innovation Studies},
year = {2024},
issn = {2096-2487},
doi = {https://doi.org/10.1016/j.ijis.2024.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2096248724000171},
author = {an-Teng Liao and Chung-Lien Pan and Zhiying Wu},
keywords = {Digital platforms, Digital ecosystem, Digitalization, Innovation ecosystems, Open innovation, Business model, Ecosystem economy, Industry 4.0, Carbon neutrality, socio-technical transition},
abstract = {This research aims to provide an ecosystem-inspired approach to digital transformation (DT) for sustainable innovations by reviewing state-of-the-art research. Focusing on DT innovation pathways for Sustainable Development Goals (SDGs), a DT innovation roadmap incorporating various notions of ecosystems can thus be developed based on the findings in response to advancements in the “Helix” models of innovation and policy progress in green digital transformation. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol, a bibliographic search was conducted in the Web of Science (WoS) database. A bibliometric analysis was then performed, leading to various science mapping outcomes. The research findings suggest practical DT innovation pathways for stakeholders aiming to achieve SDGs and a DT innovation roadmap to assist researchers in navigating the evolving innovation landscape. Since 2007, research output has experienced rapid growth, especially after 2018, with active contributions from countries such as China, Australia, England, and Italy, as well as affiliations such as Guangzhou Nanfang College, Curtin University, and Harvard University. The findings, including a performance overview, a set of clustering analyses of concepts and themes, the conceptual and intellectual structure, and an SDG identification table, provide the basis for the proposed Green Digital Transformation (GDT) innovation roadmap with DT innovation pathways. This research is the first to conduct a bibliometric analysis and science mapping of the DT literature focusing on innovation and business ecosystems. The proposed GDT innovation roadmap outlines the usefulness of the concept of “ecosystem innovation,” integrating the natural ecosystem, ecosystem services, ecosystem economy, business ecosystem, innovation ecosystem, and digital ecosystems, thereby calling for the eco-design of DT strategies that foster alignment and partnership across sectors and disciplines.}
}
@article{BARRIOS2022103688,
title = {Literature review and methodological framework for integration of IoT and PLM in manufacturing industry},
journal = {Computers in Industry},
volume = {140},
pages = {103688},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103688},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000859},
author = {Piers Barrios and Christophe Danjou and Benoit Eynard},
keywords = {Internet of Things, Product lifecycle management, Digital thread, Cyber-physical system, Enterprise information system},
abstract = {Since its emergence a few decades ago, Product Lifecycle Management (PLM) has mainly shaped product development and production engineering and helped achieve a tremendous quickening in processes and operations. On the other hand, Internet of Things (IoT) is currently in very strong emergence and people's interest in it is increasingly growing. Surprisingly, interaction between IoT and PLM systems are very scarce to this day. Industrialists have enabled a few connections when and where return on investment was high and certain. However, nowadays, the struggle for systems integration remains as the culture difference between PLM’s engineering background and IoT’s computer science background remains. This research work aims to bridge the gap by making explicit all previous research on PLM and IoT through a systematic literature review that explicits IoT’s evolving perimeter over the latest decade. It also tackles literature’s approach between the PLM & IoT information systems and humans. It finally proposes and discusses a framework supporting the integration of PLM and IoT in manufacturing industry.}
}
@article{WOLFERT2023103558,
title = {Digital innovation ecosystems in agri-food: design principles and organizational framework},
journal = {Agricultural Systems},
volume = {204},
pages = {103558},
year = {2023},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2022.103558},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X22001949},
author = {Sjaak Wolfert and Cor Verdouw and Lan {van Wassenaer} and Wilfred Dolfsma and Laurens Klerkx},
keywords = {Digital agriculture, Digital transformation, Innovation organization, Innovation infrastructure, Business ecosystems},
abstract = {CONTEXT
Digital technologies nowadays play a major role in innovation within the agri-food domain. The evolution of IT systems has currently arrived at a level that involves complex systems integration and business ecosystems in which many stakeholders in different roles are involved. A new paradigm for digital innovation is needed that copes with this increased complexity.
OBJECTIVE
This paper presents an empirically informed framework for analysing and designing viable, sustainable digital innovation ecosystems in the agri-food domain.
METHODS
The research is based on a series of European large-scale public-private innovation projects from 2011 to 2021 with a total budget of 73 M€. They involved hundreds of stakeholders that were developing a large number of digital solutions through which a digital innovation ecosystem for agri-food was formed. In a longitudinal study, a conceptual framework was used to analyse these projects and describe how the digital innovation ecosystem has developed. Lessons learnt are translated into a number of design principles and an organizational approach to foster digital innovation ecosystems in agri-food.
RESULTS AND CONCLUSIONS
The conceptual framework consists of 6 key concepts: (i) innovation strategy, (ii) innovation organization, (iii) innovation network that contains (iv) the innovation process and (v) the innovation object and finally (vi) an innovation infrastructure. Along these 6 concepts, lessons learnt and in total 21 design principles are derived from analysing the projects forming a basis for the organizational framework. At the core of this framework is a lean multi-actor approach to trials and use case development interacting with a set of multidisciplinary activities: (i) developing a common technical collaboration infrastructure, (ii) identifying value streams with user engagement, (iii) engaging the right partners and stakeholders at the right time supported by strategic project planning and dynamic management. The most important conclusion is that effective, successful and quick use of appropriate IT in agri-food requires that actors should not be analysed in isolation from both their technological and business environment. Another consequence is that a ‘minimal viable ecosystem’ only emerges after considerable time, resources and ingenuity is invested and may require outside (government) intervention.
SIGNIFICANCE
Results from this paper can be used both by public and private stakeholders to diagnose and improve digital innovation projects and develop viable, sustainable digital innovation ecosystems in agri-food.}
}
@article{LIU2023102580,
title = {Thermodynamics and its prediction and CALPHAD modeling: Review, state of the art, and perspectives},
journal = {Calphad},
volume = {82},
pages = {102580},
year = {2023},
issn = {0364-5916},
doi = {https://doi.org/10.1016/j.calphad.2023.102580},
url = {https://www.sciencedirect.com/science/article/pii/S0364591623000524},
author = {Zi-Kui Liu},
abstract = {Thermodynamics is a science concerning the state of a system, whether it is stable, metastable, or unstable, when interacting with its surroundings. The combined law of thermodynamics derived by Gibbs about 150 years ago laid the foundation of thermodynamics. In Gibbs combined law, the entropy production due to internal processes was not included, and the 2nd law was thus practically removed from the Gibbs combined law, so it is only applicable to systems under equilibrium, thus commonly termed as equilibrium or Gibbs thermodynamics. Gibbs further derived the classical statistical thermodynamics in terms of the probability of configurations in a system in the later 1800's and early 1900's. With the quantum mechanics (QM) developed in 1920's, the QM-based statistical thermodynamics was established and connected to classical statistical thermodynamics at the classical limit as shown by Landau in the 1940's. In 1960's the development of density functional theory (DFT) by Kohn and co-workers enabled the QM prediction of properties of the ground state of a system. On the other hand, the entropy production due to internal processes in non-equilibrium systems was studied separately by Onsager in 1930's and Prigogine and co-workers in the 1950's. In 1960's to 1970's the digitization of thermodynamics was developed by Kaufman in the framework of the CALculation of PHAse Diagrams (CALPHAD) modeling of individual phases with internal degrees of freedom. CALPHAD modeling of thermodynamics and atomic transport properties has enabled computational design of complex materials in the last 50 years. Our recently termed zentropy theory integrates DFT and statistical mechanics through the replacement of the internal energy of each individual configuration by its DFT-predicted free energy. The zentropy theory is capable of accurately predicting the free energy of individual phases, transition temperatures and properties of magnetic and ferroelectric materials with free energies of individual configurations solely from DFT-based calculations and without fitting parameters, and is being tested for other phenomena including superconductivity, quantum criticality, and black holes. Those predictions include the singularity at critical points with divergence of physical properties, negative thermal expansion, and the strongly correlated physics. Those individual configurations may thus be considered as the genomic building blocks of individual phases in the spirit of the materials genome®. This has the potential to shift the paradigm of CALPHAD modeling from being heavily dependent on experimental inputs to becoming fully predictive with inputs solely from DFT-based calculations and machine learning models built on those calculations and existing experimental data through newly developed and future open-source tools. Furthermore, through the combined law of thermodynamics including the internal entropy production, it is shown that the kinetic coefficient matrix of independent internal processes is diagonal with respect to the conjugate potentials in the combined law, and the cross phenomena that the phenomenological Onsager flux and reciprocal relationships are due to the dependence of the conjugate potential of a molar quantity on nonconjugate molar quantities and other potentials, which can be predicted by the zentropy theory and CALPHAD modeling.}
}
@article{CHEN201917,
title = {Assessing Software Understandability in Systems by Leveraging Fuzzy Method and Linguistic Analysis},
journal = {Procedia Computer Science},
volume = {153},
pages = {17-26},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307100},
author = {Celia Chen and Michael Shoga and Brian Li and Barry Boehm},
keywords = {software understandability, natural language processing, empirical study},
abstract = {Many if not most software-intensive systems have their mission-critical software modified by people other than its developers. The resulting misunderstandings often seriously compromise the missions the system is supporting (over 80% of the functionality in most current ground, sea, air, and space vehicles depends on software). There is a major need for models, methods, processes, and tools for identifying sources and effects of software misunderstandings, both in preparation for the system’s software use and evolution, and in evaluating modified software to avoid further examples of misunderstanding. Emphasizing high software understandability enables system maintainers to avoid these misunderstandings as they modify existing software systems. However, while many metrics for understandability have been developed, with the majority of them being source code based, little to no correlation has been found for these metrics with actual software understandability. In this paper, we instead focus on issue summaries as a non-source code alternative for measuring understandability. We generate fuzzy rules and linguistic patterns using a sample of issue summaries from the Mozilla community and evaluate 1416 issue summaries from two other software systems to measure the performance of our model. Our results suggest that this is a viable method to measure understandability and has the potential to be extended to other software maintainability qualities.}
}
@article{CURRY2019405,
title = {A Real-time Linked Dataspace for the Internet of Things: Enabling “Pay-As-You-Go” Data Management in Smart Environments},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {405-422},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1732887X},
author = {Edward Curry and Wassim Derguech and Souleiman Hasan and Christos Kouroupetroglou and Umair {ul Hassan}},
keywords = {Smart environments, Data management, Internet of Things, Water management, Energy management, Dataspace, Linked data, Semantic web, Event processing, Distributed systems},
abstract = {As smart environments move from a research vision to concrete manifestations in real-world enabled by the Internet of Things, they are encountering a number of very practical challenges in data management in terms of the flexibility needed to bring together contextual and real-time data, the interface between new digital infrastructures and existing information systems, and how to easily share data between stakeholders in the environment. Therefore, data management approaches for smart environments need to support flexibility, dynamicity, incremental change, while keeping costs to a minimum. A Dataspace is an emerging approach to data management that has proved fruitful for personal information and scientific data management. However, their use within smart environments and for real-time data remains largely unexplored. This paper introduces a Real-time Linked Dataspace (RLD) as an enabling platform for data management within smart environments. This paper identifies common data management requirements for smart energy and water environments, details the RLD architecture and the key support services and their tiered support levels, and a principled approach to “Pay-As-You-Go” data management. The paper presents a dataspace query service for real-time data streams and entities to enable unified entity-centric queries across live and historical stream data. The RLD was validated in 5 real-world pilot smart environments following the OODA (Observe, Orient, Decide, and Act) Loop to build real-time analytics, decisions support, and smart apps for energy and water management. The pilots demonstrate that the RLD enables incremental pay-as-you-go data management with support services that simplify the development of applications and analytics for smart environments. Finally, the paper discusses experiences, lessons learnt, and future directions.}
}
@article{ESTEFO2019226,
title = {The Robot Operating System: Package reuse and community dynamics},
journal = {Journal of Systems and Software},
volume = {151},
pages = {226-242},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300342},
author = {Pablo Estefo and Jocelyn Simmonds and Romain Robbes and Johan Fabry},
keywords = {Robot Operating System, Package management, Software ecosystems},
abstract = {ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.}
}
@article{BONATI2020107516,
title = {Open, Programmable, and Virtualized 5G Networks: State-of-the-Art and the Road Ahead},
journal = {Computer Networks},
volume = {182},
pages = {107516},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107516},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620311786},
author = {Leonardo Bonati and Michele Polese and Salvatore D’Oro and Stefano Basagni and Tommaso Melodia},
keywords = {Software-defined Networking, 5G, Open Source, Network Function Virtualization, O-RAN, ONAP},
abstract = {Fifth generation (5G) cellular networks will serve a wide variety of heterogeneous use cases, including mobile broadband users, ultra-low latency services and massively dense connectivity scenarios. The resulting diverse communication requirements will demand networking with unprecedented flexibility, not currently provided by the monolithic black-box approach of 4G cellular networks. The research community and an increasing number of standardization bodies and industry coalitions have recognized softwarization, virtualization, and disaggregation of networking functionalities as the key enablers of the needed shift to flexibility. Particularly, software-defined cellular networks are heralded as the prime technology to satisfy the new application-driven traffic requirements and to support the highly time-varying topology and interference dynamics, because of their openness through well-defined interfaces, and programmability, for swift and responsive network optimization. Leading the technological innovation in this direction, several 5G software-based projects and alliances have embraced the open source approach, making new libraries and frameworks available to the wireless community. This race to open source softwarization, however, has led to a deluge of solutions whose interoperability and interactions are often unclear. This article provides the first cohesive and exhaustive compendium of recent open source software and frameworks for 5G cellular networks, with a full stack and end-to-end perspective. We detail their capabilities and functionalities focusing on how their constituting elements fit the 5G ecosystem, and unravel the interactions among the surveyed solutions. Finally, we review hardware and testbeds on which these frameworks can run, and provide a critical perspective on the limitations of the state-of-the-art, as well as feasible directions toward fully open source, programmable 5G networks.}
}
@article{RAY2023121,
title = {ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {3},
pages = {121-154},
year = {2023},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266734522300024X},
author = {Partha Pratim Ray},
keywords = {ChatGPT, Language model, GPT-3.5, Generative AI, Conversational AI, Context understanding, Natural language processing},
abstract = {In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.}
}
@article{PINHO20151190,
title = {P-SOCRATES: A parallel software framework for time-critical many-core systems},
journal = {Microprocessors and Microsystems},
volume = {39},
number = {8},
pages = {1190-1203},
year = {2015},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2015.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0141933115000836},
author = {Luís Miguel Pinho and Vincent Nélis and Patrick Meumeu Yomsi and Eduardo Quiñones and Marko Bertogna and Paolo Burgio and Andrea Marongiu and Claudio Scordino and Paolo Gai and Michele Ramponi and Michal Mardiak},
keywords = {Many-core systems, Real-time systems, Embedded systems, WCET analysis, Real-time scheduling, Parallel programming models},
abstract = {Current generation of computing platforms is embracing multi-core and many-core processors to improve the overall performance of the system, meeting at the same time the stringent energy budgets requested by the market. Parallel programming languages are nowadays paramount to extracting the tremendous potential offered by these platforms: parallel computing is no longer a niche in the high performance computing (HPC) field, but an essential ingredient in all domains of computer science. The advent of next-generation many-core embedded platforms has the chance of intercepting a converging need for predictable high-performance coming from both the High-Performance Computing (HPC) and Embedded Computing (EC) domains. On one side, new kinds of HPC applications are being required by markets needing huge amounts of information to be processed within a bounded amount of time. On the other side, EC systems are increasingly concerned with providing higher performance in real-time, challenging the performance capabilities of current architectures. This converging demand raises the problem about how to guarantee timing requirements in presence of parallel execution. The paper presents how the time-criticality and parallelisation challenges are addressed by merging techniques coming from both HPC and EC domains, and provides an overview of the proposed framework to achieve these objectives.}
}
@article{GOLDSCHMIDT201828,
title = {Container-based architecture for flexible industrial control applications},
journal = {Journal of Systems Architecture},
volume = {84},
pages = {28-36},
year = {2018},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1383762117304988},
author = {Thomas Goldschmidt and Stefan Hauck-Stattelmann and Somayeh Malakuti and Sten Grüner},
abstract = {Cyber-physical systems and the Internet-of-Things are getting more and more traction in different application areas. Boosted by initiatives such as Industrie 4.0 in Germany or the Industrial Internet Consortium in the US, they are enablers for innovation in industrial automation. To provide the advanced flexibility in production envisioned for future automation systems, Programmable Logic Controllers (PLCs), as one of their main building blocks, also need to become more flexible. However, the conservative nature of this domain prohibits changes in the controller architecture impacting the installed base. Currently there exist various approaches that evolve control architectures to the next level, but none of them address flexible function deployment at the same time with legacy support. In this paper, we present an architecture for a multi-purpose controller that is inspired by the virtualization trend in cloud systems which moves from heavyweight virtual machines to lightweight containers solutions such as LXC or Docker. Our solution includes the support for multiple PLC execution engines and adds support for the emulation of legacy engines as well. We evaluate this architecture by executing performance measurements that analyze the impact of container technologies to the real-time aspects of PLC engines.}
}
@article{BALAJI20181273,
title = {Brick : Metadata schema for portable smart building applications},
journal = {Applied Energy},
volume = {226},
pages = {1273-1292},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918302162},
author = {Bharathan Balaji and Arka Bhattacharya and Gabriel Fierro and Jingkun Gao and Joshua Gluck and Dezhi Hong and Aslak Johansen and Jason Koh and Joern Ploennigs and Yuvraj Agarwal and Mario Bergés and David Culler and Rajesh K. Gupta and Mikkel Baun Kjærgaard and Mani Srivastava and Kamin Whitehouse},
keywords = {Smart buildings, Building management, Metadata, Schema, Ontology},
abstract = {Buildings account for 32% of worldwide energy usage. A new regime of exciting new “applications” that span a distributed fabric of sensors, actuators and humans has emerged to improve building energy efficiency and operations management. These applications leverage the technological advances in embedded sensing, processing, networking and methods by which they can be coupled with supervisory control and data acquisition systems deployed in modern buildings and with users on mobile wireless platforms. There are, however, several technical challenges to confront before such a vision of smart building applications and cyber-physical systems can be realized. The sensory data produced by these systems need significant curation before it can be used meaningfully. This is largely a manual, cost-prohibitive task and hence such solutions rarely experience widespread adoption due to the lack of a common descriptive schema. Recent attempts have sought to address this through data standards and metadata schemata but fall short in capturing the richness of relationships required by applications. This paper describes Brick, a uniform metadata schema for representing buildings that builds upon recent advances in the area. Our schema defines a concrete ontology for sensors, subsystems and the relationships between them, which enables portable applications. We demonstrate the completeness and effectiveness of Brick by using it to represent the entire vendor-specific sensor metadata of six diverse buildings across different campuses, comprising 17,700 data points, and running eight unmodified energy efficiency applications on these buildings.}
}
@article{RIEGER2019175,
title = {Towards the definitive evaluation framework for cross-platform app development approaches},
journal = {Journal of Systems and Software},
volume = {153},
pages = {175-199},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300743},
author = {Christoph Rieger and Tim A. Majchrzak},
keywords = {Mobile app, Mobile computing, Cross-platform, Multi-platform, Development framework},
abstract = {Mobile app development is hindered by device fragmentation and vendor-specific modifications. Boundaries between devices blur with PC-tablet hybrids on the one side and wearables on the other. Future apps need to support a host of app-enabled devices with differing capabilities, along with their software ecosystems. Prior work on cross-platform app development concerned concepts and prototypes, and compared approaches that target smartphones. To aid choosing an appropriate framework and to support the scientific assessment of approaches, an up-to-date comparison framework is needed. Extending work on a holistic, weighted set of assessment criteria, we propose what could become the definitive framework for evaluating cross-platform approaches. We have based it on sound abstract concepts that allow extensions. The weighting capabilities offer customisation to avoid the proverbial comparison of apples and oranges lurking in the variety of available frameworks. Moreover, it advises on multiple development situations based on a single assessment. In this article, we motivate and describe our evaluation criteria. We then present a study that assesses several frameworks and compares them to Web Apps and native development. Our findings suggest that cross-platform development has seen much progress but the challenges are ever growing. Therefore, additional support for app developers is warranted.}
}
@article{ORIOL2023100808,
title = {Comprehensive assessment of open source software ecosystem health},
journal = {Internet of Things},
volume = {22},
pages = {100808},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100808},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001312},
author = {Marc Oriol and Carlos Müller and Jordi Marco and Pablo Fernandez and Xavier Franch and Antonio Ruiz-Cortés},
keywords = {Open Source Software Ecosystem (OSSECO), Open Source Software (OSS), Monitoring, Ecosystem health},
abstract = {Recent surveys expose that the use of Open Source Software (OSS) is increasingly becoming a need for organizations in their development projects. However, deciding a proper OSS to be adopted or to contribute to its development is a complex and error-prone task. Analyzing the OSS ecosystem (OSSECO) health may help providing information about: (1) the OSS itself (number of commits, days after the last release, etc.); and (2) their main actors (number of contributors, partners, etc.). There exist proposals that go further and provide aggregated high-level indicators (e.g. visibility as an aggregation of number of community events, number of partners, and other metrics). Nevertheless, there is a lack of useful OSSECO analysis tools to ease the decision making on which OSSECO has the health required by a potential OSS adopter or contributor. In this work, we provide OSS-CARE (OSSeCo heAlthy monitoR and analysEr), an OSS-independent, fully automatic, and real-time framework to assess OSSECO’s health. OSS-CARE supports defining the ecosystem health objectives of potential OSS adopters, OSS contributors, and even OSS managers to inspect their provided health. These objectives are defined based on a well-established model characterizing health metrics that can be potentially aggregated by using a Bayesian network technique. Moreover, the integrated monitoring and analysis components perform an automated assessment of OSSECO’s health by checking the fulfillment of the required health objectives. Furthermore, the result is shown in an appealing dashboard that may ease the complex decision making of which OSS to choose.}
}
@article{TRIPATHI2024107424,
title = {Stakeholders collaborations, challenges and emerging concepts in digital twin ecosystems},
journal = {Information and Software Technology},
volume = {169},
pages = {107424},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107424},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000296},
author = {Nirnaya Tripathi and Heidi Hietala and Yueqiang Xu and Reshani Liyanage},
keywords = {Digital twin, Digital twin ecosystem, Stakeholders, Systematic literature review, Empirical study, Definition, Software development},
abstract = {Context:
Digital twin (DT) ecosystems are rapidly evolving, connecting many stakeholders, such as manufacturers, customers, and application platform providers. These ecosystems require collaboration and interaction between diverse actors to create value. This study delves into the collaboration of such stakeholders within DT-focused ecosystems.
Objective:
This research aims to understand stakeholder collaboration within DT ecosystems, identify potential challenges, and provide insights for managing these stakeholders. It also seeks to define the DT ecosystem and its implications for both research and practice.
Method:
A systematic literature review was conducted, supplemented by empirical evidence gathered from interviews with DT experts who were knowledgeable about the DT ecosystem. The study also analyzed DT systems, stakeholder roles, and the challenges with ecosystem-focused DT development.
Results:
The study identified various stakeholders and their roles in adding value to a DT ecosystem. It highlighted the benefits of stakeholder collaboration, such as knowledge gain during DT system development. The research also revealed the technical and non-technical challenges encountered in ecosystem-focused DTs, emphasizing the importance of standardization as a solution. A new definition of the DT ecosystem was proposed, emphasizing its data-driven nature, interconnected DTs, stakeholder value creation, and technology enablement.
Conclusion:
Stakeholder collaboration is pivotal in DT ecosystems, with each actor playing a distinct role. Addressing challenges, especially through standardization (OPC UA and ISO 23247), can lead to more efficient and coherent DT ecosystems. The insights provided by this study can guide industries in designing, developing, and maintaining their DT ecosystems, ensuring value creation and stakeholder satisfaction. Future research avenues that emphasize the importance of understanding the challenges involved and deploy appropriate solutions were suggested.}
}
@article{ALAA201748,
title = {A review of smart home applications based on Internet of Things},
journal = {Journal of Network and Computer Applications},
volume = {97},
pages = {48-65},
year = {2017},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517302801},
author = {Mussab Alaa and A.A. Zaidan and B.B. Zaidan and Mohammed Talal and M.L.M. Kiah},
keywords = {Smart home application, Remote home, Intelligent home, Home automation system, Automated home, Internet of Things (IoT)},
abstract = {The new and disruptive technology of smart home applications (hereafter referred to as apps) based on Internet of Things (IoT) is largely limited and scattered. To provide valuable insights into technological environments and support researchers, we must understand the available options and gaps in this line of research. Thus, in this study, a review is conducted to map the research landscape into a coherent taxonomy. We conduct a focused search for every article related to (1) smart homes, (2) apps, and (3) IoT in three major databases, namely, Web of Science, ScienceDirect, and IEEE Explore. These databases contain literature focusing on smart home apps using IoT. The final dataset resulting from the classification scheme includes 229 articles divided into four classes. The first class comprises review and survey articles related to smart home IoT applications. The second class includes papers on IoT applications and their use in smart home technology. The third class contains proposals of frameworks to develop and operate applications. The final class includes studies with actual attempts to develop smart home IoT applications. We then identify the basic characteristics of this emerging field in the following aspects: motivation of using IoT in smart home applications, open challenges hindering utilization, and recommendations to improve the acceptance and use of smart home applications in literature.}
}
@article{YANG2021100088,
title = {Implementation for a cloud battery management system based on the CHAIN framework},
journal = {Energy and AI},
volume = {5},
pages = {100088},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000422},
author = {Shichun Yang and Zhengjie Zhang and Rui Cao and Mingyue Wang and Hanchao Cheng and Lisheng Zhang and Yinan Jiang and Yonglin Li and Binbin Chen and Heping Ling and Yubo Lian and Billy Wu and Xinhua Liu},
keywords = {Battery, CHAIN, Cloud, Battery management system, SOX estimation, end-edge-cloud architecture},
abstract = {Summary
An intelligent battery management system is a crucial enabler for energy storage systems with high power output, increased safety and long lifetimes. With recent developments in cloud computing and the proliferation of big data, machine learning approaches have begun to deliver invaluable insights, which drives adaptive control of battery management systems (BMS) with improved performance. In this paper, a general framework utilizing an end-edge-cloud architecture for a cloud-based BMS is proposed, with the composition and function of each link described. Cloud-based BMS leverages from the Cyber Hierarchy and Interactional Network (CHAIN) framework to provide multi-scale insights, more advanced and efficient algorithms can be used to realize the state-of-X estimation, thermal management, cell balancing, fault diagnosis and other functions of traditional BMS system. The battery intelligent monitoring and management platform can visually present battery performance, store working-data to help in-depth understanding of the microscopic evolutionary law, and provide support for the development of control strategies. Currently, the cloud-based BMS requires more effects on the multi-scale integrated modeling methods and remote upgrading capability of the controller, these two aspects are very important for the precise management and online upgrade of the system. The utility of this approach is highlighted not only for automotive applications, but for any battery energy storage system, providing a holistic framework for future intelligent and connected battery management.}
}
@article{YE2023107287,
title = {MDSSED: A safety and security enhanced model-driven development approach for smart home apps},
journal = {Information and Software Technology},
volume = {163},
pages = {107287},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107287},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001416},
author = {Tong Ye and Yi Zhuang and Gongzhe Qiao},
keywords = {Security, Safety, Smart home apps, Model-driven software development, Formal method},
abstract = {Context:
With the popularization of smart home devices, people rely more on automation functions provided by smart home apps. This increases the attack surface for safety and security threats. Many of these threats are at the interaction level, caused by unintended or malicious interactions between apps.
Objective:
Most of the current studies focus on identifying unsafe interactions between smart home apps by code analysis. To the best of our knowledge, none of the existing studies focuses on enhancing the safety and security of smart home apps under interaction threats in the design phase. To fill this gap, this paper presents MDSSED, a safety and security enhanced model-driven development approach for smart home apps.
Method:
First, this paper identifies eleven types of interaction threats faced by smart home apps. Second, the MDSSED profile is proposed to support modeling smart home apps using UML. Third, the MDSSED prototype tool is developed to generate threat models and corresponding safety and security properties automatically. Then, the safety and security properties are automatically verified by model checking. Finally, the MDSSED tool automatically converts the UML models to the Samsung SmartThings apps.
Results:
To evaluate the accuracy and effectiveness of MDSSED, this paper uses the benchmarks in existing state-of-the-art studies. The results show that MDSSED not only identified the safety and security problems in the existing benchmarks but also pointed out vulnerabilities of apps under other interaction threats identified in this paper.
Conclusion:
To the best of our knowledge, MDSSED is the first model-driven development approach that supports the automatic verification of the safety and security properties of smart home apps under interaction threats. The accuracy, practicality, and efficiency of MDSSED are corroborated by experiments. The source code of the MDSSED tool and the experimental data are available online.11https://github.com/YETONG1219/MDSSED.}
}
@article{ELKADIRI201614,
title = {Current trends on ICT technologies for enterprise information systems},
journal = {Computers in Industry},
volume = {79},
pages = {14-33},
year = {2016},
note = {Special Issue on Future Perspectives On Next Generation Enterprise Information Systems},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2015.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166361515300142},
author = {Soumaya {El Kadiri} and Bernard Grabot and Klaus-Dieter Thoben and Karl Hribernik and Christos Emmanouilidis and Gregor {von Cieminski} and Dimitris Kiritsis},
keywords = {Enterprise information system, Manufacturing, ICT, Data value chain, Context awareness, Usability, Human learning},
abstract = {As business conditions change rapidly, the need for integrating business and technical systems calls for novel ICT frameworks and solutions to remain concurrent in highly competitive markets. A number of problems and issues arise in this regard. In this paper, four big challenges of enterprise information systems (EIS) are defined and discussed: (1) data value chain management; (2) context awareness; (3) usability, interaction and visualization; and (4) human learning and continuous education. Major contributions and research orientations of ICT technologies are elaborated based on selected key issues and lessons learned. First, the semantic mediator is proposed as a key enabler for dealing with semantic interoperability. Second, the context-aware infrastructures are proposed as a main solution for making efficient use of EIS to offer a high level of customization of delivered services and data. Third, the product avatar is proposed as a contribution to an evolutionary social, collaborative and product-centric and interaction metaphor with EIS. Fourth, human learning solutions are considered to develop individual competences in order to cope with new technological advances. The paper ends with a discussion on the impact of the proposed solutions on the economic and social landscape and proposes a set of recommendations as a perspective towards next generation of information systems.}
}
@article{AI201877,
title = {Edge computing technologies for Internet of Things: a primer},
journal = {Digital Communications and Networks},
volume = {4},
number = {2},
pages = {77-86},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817301335},
author = {Yuan Ai and Mugen Peng and Kecheng Zhang},
keywords = {Internet of Things (IoT), Mobile edge computing, Cloudlets, Fog computing},
abstract = {With the rapid development of mobile internet and Internet of Things applications, the conventional centralized cloud computing is encountering severe challenges, such as high latency, low Spectral Efficiency (SE), and non-adaptive machine type of communication. Motivated to solve these challenges, a new technology is driving a trend that shifts the function of centralized cloud computing to edge devices of networks. Several edge computing technologies originating from different backgrounds to decrease latency, improve SE, and support the massive machine type of communication have been emerging. This paper comprehensively presents a tutorial on three typical edge computing technologies, namely mobile edge computing, cloudlets, and fog computing. In particular, the standardization efforts, principles, architectures, and applications of these three technologies are summarized and compared. From the viewpoint of radio access network, the differences between mobile edge computing and fog computing are highlighted, and the characteristics of fog computing-based radio access network are discussed. Finally, open issues and future research directions are identified as well.}
}
@article{GUPTA2019100,
title = {Business, innovation and digital ecosystems landscape survey and knowledge cross sharing},
journal = {Technological Forecasting and Social Change},
volume = {147},
pages = {100-109},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518313301},
author = {Ranjit Gupta and Cristian Mejia and Yuya Kajikawa},
keywords = {Business ecosystems, Innovation ecosystems, Digital ecosystems, Bibliometric analysis, Knowledge management},
abstract = {Business Ecosystem (BE) has been defined in multiple ways and has been used interchangeably, jointly and overlapping with innovation ecosystems (IE) and digital ecosystems (DE), making it difficult to differentiate, consolidate, utilize and grow the body of knowledge, both in academia and industry. We use text mining techniques to reveal overlapping and exclusive terminologies used in academic articles on the three types of ecosystems. Processing of keywords has identified the current positioning of related domains in a tripartite framework. Keywords Network Analysis has revealed domains that share knowledge with the larger domains of Smart City, Digital Business Ecosystem (DBE) and Helix interaction concepts, thereby providing insights into knowledge sharing across boundaries of BE, IE and DE. The results offer a basis for classification and baseline of current state of knowledge distribution across the ecosystems that can aid in ecosystems design, collaborations planning, interdisciplinary research and policy making.}
}
@article{FERNANDEZ2021100408,
title = {The design of secure IoT applications using patterns: State of the art and directions for research},
journal = {Internet of Things},
volume = {15},
pages = {100408},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000524},
author = {Eduardo B. Fernandez and Hironori Washizaki and Nobukazu Yoshioka and Takao Okubo},
keywords = {IoT applications, IoT systems design, Internet of Things, Security patterns, Misuse patterns, Privacy patterns, Reference architectures, Secure systems development, Microservices, IoT survey},
abstract = {Internet of Things (IoT) systems are exposed to a large variety of threats due to the inclusion of many devices which may have different owners and manufacturers. IoT applications often include parts in clouds and fogs as well as being part of larger cyber-physical systems; that is, these systems are very complex, which also contributes to their security problems. The design of IoT-based applications must be able to handle this complexity and heterogeneity; patterns are a good approach for this purpose because of their abstraction power. When using patterns, a good catalog is necessary. We survey and classify existing IoT security patterns to see their coverage and quality to evaluate how appropriate they are to be part of a useful catalog. A practical catalog must cover most of the standard security mechanisms. Pattern descriptions include several sections according to a template. We conclude that the number of existing patterns is insufficient for a working catalog and most of them are incomplete or use different descriptions; we need to build a unified catalog. We have started in that direction by creating new patterns or rewriting existing patterns to make them follow a common description. To use the patterns, we need a secure development methodology and we survey IoT development methodologies; we find that none of them considers security or uses patterns. As a solution, we propose modifying existing pattern-based methodologies for distributed systems, of which there is a good variety, using one of them as reference for concreteness. We provide a list of possible research directions about these topics.}
}
@article{HAGHIGHATKHAH201725,
title = {Automotive software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {128},
pages = {25-55},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300560},
author = {Alireza Haghighatkhah and Ahmad Banijamali and Olli-Pekka Pakanen and Markku Oivo and Pasi Kuvaja},
keywords = {Literature survey, Systematic mapping study, Automotive software engineering, Automotive systems, Embedded systems, Software-intensive systems},
abstract = {The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.}
}
@article{AXELSSON201669,
title = {Quality assurance in software ecosystems: A systematic literature mapping and research agenda},
journal = {Journal of Systems and Software},
volume = {114},
pages = {69-81},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215002861},
author = {Jakob Axelsson and Mats Skoglund},
keywords = {Software ecosystems, Quality, Verification, Testing},
abstract = {Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users.}
}
@article{MOURTZIS2022653,
title = {Human centric platforms for personalized value creation in metaverse},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {653-659},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001959},
author = {Dimitris Mourtzis and Nikos Panopoulos and John Angelopoulos and Baicun Wang and Lihui Wang},
keywords = {Metaverse, Personalization, Industry 5.0, Society 5.0, Human Cyber-Physical Systems (HCPS)},
abstract = {The term “Metaverse” first used in Neal Stephenson's sci-fi book Snow Crash in 1992, refers to a fusion of virtual and real existence. Nearly 30 years later, that definition is taking shape and promises to alter how people live and operate. This next evolution of Internet also known as Web3.0 will combine digital and physical elements. Multiple definitions can be found in the literature, with the most prevalent being the “new internet”, among others such as “democratized virtual society”, “persistent virtual spaces”, “a digital twin of our own world for personalized value creation”. Consequently, the common consensus dictates that Metaverse can be realized as a new form of the Internet, totally reshaped from what is already known. As we are heading towards the coexistence of Industry 5.0 and Society 5.0 (super smart and intelligent society), this paper attempts to present the definition of Metaverse, its evolution, the advantages and disadvantages, the pillars for the technological advancement which could be the fuel to spark future investigation and discussion as well as to accelerate the development of Metaverse towards the human centric and personalized society. Furthermore, in this manuscript, challenges and opportunities are presented (including Manufacturing), a brief comparison is performed versus Virtual Reality, and a conceptual framework for integrating Metaverse in Manufacturing is also presented.}
}
@incollection{CHRISTEN202045,
title = {Chapter 3 - Hypergraph-based type theory for software development in a Cyber-Physical context},
editor = {Amy Neustein},
booktitle = {Advances in Ubiquitous Computing},
publisher = {Academic Press},
pages = {45-138},
year = {2020},
series = {Advances in ubiquitous sensing applications for healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-816801-1.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168011000037},
author = {Nathaniel Christen},
keywords = {Hypergraphs, Requirements Engineering, Procedural data modeling, Software language engineering, Semantic web, Scientific data sharing},
abstract = {This chapter will explore the integration of several methodologies related to source code analysis and Requirements Engineering in a software development context. The chapter will review graph-based representations of source code, alongside applied type theory (for expressing programming languages’ type systems) and systematic accounts of foundational programming elements such as functions/procedures, function calls, and inter-procedure information flows. The new representational device described here involves a theory of “channels” which permits graph-based code models to be integrated with type theories and lambda calculi structured around modern programming paradigms. The proposed techniques support documentation and verification of procedural, data type, and holistic specifications—implementational assumptions on procedures and/or modeling assumptions on types, along with larger-scale inter-type relationships. This chapter will use real-world Cyber-Physical case studies to illustrate data models which call for advanced code-documentation techniques, insofar as Cyber-Physical software should prioritize safety and reliability. For concrete examples, an accompanying open-source dataset (at https://github.com/scignscape/ntxh) demonstrates code libraries concretizing techniques outlined here.}
}
@article{JIANG2023101951,
title = {Multi-domain ubiquitous digital twin model for information management of complex infrastructure systems},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101951},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101951},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000794},
author = {Yishuo Jiang and Ming Li and Wei Wu and Xiqiang Wu and Xiaoning Zhang and Xinyan Huang and Ray Y. Zhong and George G.Q. Huang},
keywords = {Digital twin, Ubiquitous model, Information management, Industrial infrastructure systems, Transdisciplinary management, Domain-Driven Design (DDD)},
abstract = {Digital twin is a comprehensive digital equivalent of an object or an activity, reflecting the semantic and geometric properties and behaviors through virtual models and data. Digital twin and related information technologies are widely used in the construction, operation and maintenance of infrastructure and facility. Transdisciplinary stakeholders are always involved in the long-term and cross-scene management of IoT and digital twin-enabled smart infrastructures and facilities. The intensive interactions among stakeholders often cause conflicts due to the variations in experience, knowledge, and interests. Moreover, with the change propagation of digital twins, cyber-physical resources can’t be efficiently and consistently established, connected, and utilized with multi-domain information through selective simplification and structured methods. This paper proposes a Ubiquitous Digital Twin model for the information management of complex infrastructure systems based on Domain-Driven Design. To achieve the unified and structured description, six domains are deployed in the proposed model with sequential or parallel tuples for shared understanding of overall system framework or specific functional modules. Three cases of one smart nuclear plant management scenario are hierarchically instantiated to evaluate the proposed model.}
}
@incollection{POSEY2018153,
title = {6 - Infrastructure for Transportation Cyber-Physical Systems},
editor = {Lipika Deka and Mashrur Chowdhury},
booktitle = {Transportation Cyber-Physical Systems},
publisher = {Elsevier},
pages = {153-171},
year = {2018},
isbn = {978-0-12-814295-0},
doi = {https://doi.org/10.1016/B978-0-12-814295-0.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012814295000006X},
author = {Brandon Posey and Linh Bao Ngo and Mashrur Chowdhury and Amy Apon},
keywords = {Cyber-physical systems, Data collection, Hadoop MapReduce, HDFS, Lambda architecture, RDBMS},
abstract = {In transportation cyber-physical systems (TCPS), data are collected from different transportation modes and from a wide variety of data collection devices. TCPS data have characteristics that can be described using ‘5Vs of big data’: (1) volume, (2) variety, (3) velocity, (4) veracity and (5) value (Demchenko et al., 2013) . Volume represents the raw amount of data to be stored. Velocity represents the rate at which data are being collected and inserted into the data infrastructure and how fast the back end services are expected to process these data streams. The variety aspect arises from the fact that in a data-driven environment, it is inevitable for service providers to collect and combine data from numerous different sources for creating actionable insights. As a result, data will come into the infrastructure under different data formats. With increases in data size, rate of data arrival and number of data sources, the reliability of data contents will certainly decrease. This is the veracity aspect. The final V, value, represents the desired outcome for investing in TCPS data, because all efforts are for nought if usable insights cannot be generated from the data. All of these characteristics make traditional database management systems and conventional data processing and delivery systems inappropriate for many types of transportation data analysis and decision support tasks. An enormous amount of heterogeneous data cannot be processed in real time. This chapter introduces the modern data infrastructures that are needed to support TCPS. The focus is on open-source hardware and software technologies that support data management and delivery systems. The chapter also includes a discussion of infrastructure as code that can leverage emerging commercial cloud facilities.}
}
@article{ZHAO2020161,
title = {Reconstructing CNC platform for EDM machines towards smart manufacturing},
journal = {Procedia CIRP},
volume = {95},
pages = {161-177},
year = {2020},
note = {20th CIRP CONFERENCE ON ELECTRO PHYSICAL AND CHEMICAL MACHINING},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.134},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120309860},
author = {Wansheng Zhao and Mo Chen and Weiwen Xia and Xuecheng Xi and Fuchun Zhao and Yaou Zhang},
keywords = {open architecture CNC platform, EDM, die-sinking EDM, small hole fast drilling EDM, wire EDM, smart manufacturing, openness, standardization, generalized unit arc length increment method, digitizer/player architecture, machining process adaptive control, application of AI},
abstract = {CNC (computer numerical control) systems play an ultimately important role for controlling EDM (electrical discharge machining) machine tools and their machining processes. Till now, existing CNC systems do not offer sufficient openness that supports researchers and engineers to expend its capabilities and functionalities in response to the increasing demands of smart manufacturing; on the other hand, transforming an EDM machine made by small and medium-sized machine tool builders, into a smart manufacturing system has never been an easy job. To address the issues and overcome the difficulties which block the way towards smart manufacturing, this paper proposes an open architecture CNC platform for EDM machine tools. This platform utilizes the state-of-the-art technologies in implementation of the hardware and software without compromising with the constraints of obsolete techniques. For demonstrating the unique capabilities, the generalized unit arc length increment (GUALI) interpolation method and the Digitizer/Player system architecture are adopted. To exhibit the feasibilities of the newly developed platform, three kinds of EDM machine tools are applied associated with advanced functionalities such as machining process adaptive control, applications of machine learning, 6-axis EDM of shrouded turbine blisks etc. In addition, a small-scale smart manufacturing unit for drilling film cooling holes of turbine blades is built up into real production by applying the new CNC system and related software applications. From the practitioner’s viewpoint, openness and standardization are the keys that enable the people from academia and industry bringing in their domain knowledge to enrich the smart manufacturing ecosystem.}
}
@article{GOU202245,
title = {Mapping series-parallel streaming applications on hierarchical platforms with reliability and energy constraints},
journal = {Journal of Parallel and Distributed Computing},
volume = {163},
pages = {45-61},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000211},
author = {Changjiang Gou and Anne Benoit and Mingsong Chen and Loris Marchal and Tongquan Wei},
keywords = {Series-parallel streaming applications, Task mapping, Hierarchical platforms, Reliability, Energy},
abstract = {Streaming applications come from various application fields such as physics, where data is continuously generated and must be processed on the fly. Typical streaming applications have a series-parallel dependence graph, and they are processed on a hierarchical failure-prone platform, as for instance in miniaturized satellites. The goal is to minimize the energy consumed when processing each data set, while ensuring real-time constraints in terms of processing time. Dynamic voltage and frequency scaling (DVFS) is used to reduce the energy consumption, and we ensure a reliable execution by either executing a task at maximum speed, or by triplicating it, so that the time to execute a data set without failure is bounded. We propose a structure rule to partition the series-parallel applications and map the application onto the platform, and we prove that the optimization problem is NP-complete. We design a dynamic-programming algorithm for the special case of linear chains, which is optimal for a special class of schedules. Furthermore, this algorithm provides an interesting heuristic and a building block for designing heuristics for the general case. The heuristics are compared to a baseline solution, where each task is executed at maximum speed. Simulations on realistic settings demonstrate the good performance of the proposed heuristics; in particular, significant energy savings can be obtained.}
}
@article{KHAN2019396,
title = {Landscaping systematic mapping studies in software engineering: A tertiary study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {396-436},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302784},
author = {Muhammad Uzair Khan and Salman Sherin and Muhammad Zohaib Iqbal and Rubab Zahid},
keywords = {Tertiary study, Systematic mapping study, Secondary study, Survey, Software engineering},
abstract = {Context
A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.
Objectives
We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.
Methodology
We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).
Results
We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.
Conclusion
There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}
@article{LEVY2023111792,
title = {Sustaining human health: A requirements engineering perspective},
journal = {Journal of Systems and Software},
volume = {204},
pages = {111792},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111792},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001875},
author = {Meira Levy and Eduard C. Groen and Kuldar Taveter and Daniel Amyot and Eric Yu and Lin Liu and Ita Richardson and Maria Spichkova and Alexandra Jussli and Sébastien Mosser},
keywords = {Requirements engineering, Health, Well-being, Sustainability},
abstract = {In our current day and age, Earth suffers under the human ecological footprint, which influences our health and well-being. Technological solutions, including software-related ones, may help tackle these concerns for humanity. However, the development of such solutions requires special attention and effort to overcome human, public, and social barriers that might prevent them from being effective. The Requirements Engineering for Well-Being, Aging, and Health (REWBAH) workshop gathering in 2021 focused on addressing the challenge of how Requirements Engineering (RE) knowledge and practices can be applied to the development of information systems that support and promote long-lasting, sustained, and healthier behavior and choices by individuals. An interactive discussion among subject matter experts and practitioners participating in the REWBAH’21 revolved around several questions. In a subsequent qualitative analysis, the emerging themes were arranged in the sustainable-health RE (SusHeRE) framework to describe RE processes that address both sustainability and health goals. In this vision paper, we present our framework, which includes four main SusHeRE goals defined according to the changes in RE that we deem necessary for achieving a positive contribution of RE on sustainability and health. These goals involve improved RE Techniques, Multidisciplinary Expertise, Education Agenda, and Public and Social Ecology.}
}
@article{LAND20207827,
title = {Selecting Test Cases for Mechatronic Products with a Variant and Version Management Approach based on a Consistent Toolchain},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {7827-7832},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1884},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320324988},
author = {K. Land and B. Vogel-Heuser and A. Gallasch and M. Sagerer and D. Förster and G. Strobl},
keywords = {Logical design, physical design, implementation of embedded computer systems, Product Variance, Computer-aided testing},
abstract = {The number of variants and versions for mechatronic products increases. The high variability poses a challenge for test engineers in selecting suitable test cases upon a change. If a requirement or a feature of a mechatronic product changes, it is not necessary to retest the whole product but only the changed parts. To identify the product features that are directly or indirectly affected by the change, a connection of test, requirement, and variant management is necessary. Therefore, an approach to select test cases based on an occurred change using variant and version knowledge is needed. In this paper, such an approach and its possible application in a toolchain are introduced. The toolchain is built by combining established tools developed by the Parametric Technology Corporation (PTC) that are already used to manage parts of the product life cycle. The resulting PTC Integrity Toolchain and the applicability of the concept on it were evaluated together with industrial experts with positive results.}
}
@article{DELAVARA2021110812,
title = {Assurance and certification of cyber–physical systems: The AMASS open source ecosystem},
journal = {Journal of Systems and Software},
volume = {171},
pages = {110812},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110812},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302120},
author = {Jose Luis {de la Vara} and Alejandra Ruiz and Gaël Blondelle},
keywords = {AMASS, Open source, Ecosystem, Assurance, Certification, Cyber–physical​ system},
abstract = {Many cyber–physical systems (CPS) are subject to rigorous assurance and certification processes to provide confidence that undue risks are not posed and thus the systems are trustworthy. These processes are complex and time-consuming and tool support can greatly aid in their execution. In line with other trends for systems and software engineering, the need for and interest in open source tools for assurance and certification is growing and different initiatives have been launched. As a concrete example, we report on our experience in developing the AMASS open source ecosystem. This ecosystem includes (1) an open source tool platform that supports the main CPS assurance and certification activities, (2) external tools with added-value features, and (3) an open community of developers and users. The platform integrates existing solutions for system modelling, process engineering, and compliance and argumentation management. We also present the application of the AMASS tool platform in 11 industrial case studies from five different application domains. The results show that the platform is a feasible means for CPS assurance and certification and that practitioners find benefits in assurance-oriented system modelling and in integrated system assurance information, among other areas. Nonetheless, improvement opportunities also exist, most notably regarding tool interoperability and usability.}
}
@article{GERALDI2020106293,
title = {Software product line applied to the internet of things: A systematic literature review},
journal = {Information and Software Technology},
volume = {124},
pages = {106293},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300434},
author = {Ricardo Theis Geraldi and Sheila Reinehr and Andreia Malucelli},
keywords = {Internet of things, Software product line, Variability management, Product family engineering, Families of systems},
abstract = {Context
Internet of Things (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. SPL engineering consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities.
Objective
This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out.
Method
A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria.
Results
The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.}
}