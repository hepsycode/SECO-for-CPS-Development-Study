{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "782e9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/derekgreene/topic-model-tutorial/blob/master/1%20-%20Text%20Preprocessing.ipynb\n",
    "from pathlib import Path\n",
    "import operator, joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27a78e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 28 raw text documents\n"
     ]
    }
   ],
   "source": [
    "in_path = Path(\"data\") / \"articles.txt\"\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "with open(in_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append( text )\n",
    "        # keep a short snippet of up to 100 characters as a title for each article\n",
    "        snippets.append(text[0:min(len(text),100)])\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c809006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 1298 entries\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\", encoding=\"utf8\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append(line.strip())\n",
    "# note that we need to make it hashable\n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51ece8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28 X 78 document-term matrix\n"
     ]
    }
   ],
   "source": [
    "# use a custom stopwords list, set the minimum term-document frequency to 20\n",
    "vectorizer = CountVectorizer(stop_words = custom_stop_words, min_df=5)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e78db32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 78 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms =  list(vectorizer.get_feature_names_out())\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76de1589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-raw.pkl']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((A,terms,snippets), \"articles-raw.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96533d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28 X 78 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 5)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e6cfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 78 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms =  list(vectorizer.get_feature_names_out())\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aeb6e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_terms(A, term):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e16132f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. systems (4.69)\n",
      "02. software (3.92)\n",
      "03. cps (3.18)\n",
      "04. ecosystems (3.09)\n",
      "05. data (2.73)\n",
      "06. cyber (2.67)\n",
      "07. ecosystem (2.66)\n",
      "08. physical (2.61)\n",
      "09. digital (2.54)\n",
      "10. innovation (2.34)\n",
      "11. engineering (2.26)\n",
      "12. computing (1.74)\n",
      "13. analysis (1.71)\n",
      "14. challenges (1.68)\n",
      "15. platform (1.60)\n",
      "16. based (1.51)\n",
      "17. development (1.51)\n",
      "18. smart (1.48)\n",
      "19. applications (1.44)\n",
      "20. internet (1.44)\n",
      "21. cloud (1.43)\n",
      "22. devices (1.43)\n",
      "23. study (1.41)\n",
      "24. model (1.39)\n",
      "25. models (1.38)\n",
      "26. time (1.25)\n",
      "27. literature (1.24)\n",
      "28. approach (1.22)\n",
      "29. paper (1.18)\n",
      "30. services (1.18)\n",
      "31. systematic (1.14)\n",
      "32. support (1.12)\n",
      "33. interoperability (1.11)\n",
      "34. provide (1.11)\n",
      "35. industry (1.05)\n",
      "36. design (1.03)\n",
      "37. opportunities (0.99)\n",
      "38. solutions (0.99)\n",
      "39. complex (0.97)\n",
      "40. management (0.97)\n",
      "41. tools (0.97)\n",
      "42. dynamic (0.94)\n",
      "43. potential (0.92)\n",
      "44. platforms (0.92)\n",
      "45. evolution (0.85)\n",
      "46. architecture (0.85)\n",
      "47. level (0.85)\n",
      "48. future (0.85)\n",
      "49. increasingly (0.82)\n",
      "50. process (0.77)\n",
      "51. business (0.77)\n",
      "52. industrial (0.77)\n",
      "53. multiple (0.74)\n",
      "54. critical (0.74)\n",
      "55. users (0.71)\n",
      "56. application (0.71)\n",
      "57. testing (0.71)\n",
      "58. components (0.69)\n",
      "59. developing (0.69)\n",
      "60. complexity (0.68)\n",
      "61. enabling (0.67)\n",
      "62. emerging (0.67)\n",
      "63. environment (0.64)\n",
      "64. requires (0.62)\n",
      "65. existing (0.60)\n",
      "66. providing (0.58)\n",
      "67. requirements (0.58)\n",
      "68. propose (0.57)\n",
      "69. technical (0.56)\n",
      "70. insights (0.56)\n",
      "71. role (0.56)\n",
      "72. domains (0.56)\n",
      "73. capabilities (0.53)\n",
      "74. traditional (0.53)\n",
      "75. actors (0.50)\n",
      "76. technologies (0.50)\n",
      "77. context (0.48)\n",
      "78. common (0.44)\n"
     ]
    }
   ],
   "source": [
    "ranking = rank_terms(A, terms)\n",
    "for i, pair in enumerate(ranking[0:100]):\n",
    "    print( \"%02d. %s (%.2f)\" % (i+1, pair[0], pair[1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cea93fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-tfidf.pkl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((A,terms,snippets), \"articles-tfidf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac34c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
